{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Logistic Regression with Replicated Data\n",
    "This notebook reads the data splits from `04_create_data_splits` to apply a Logistic Regression model in Python and a from-scratch Logistic Regression model in Spark (with a homemade implementation of Gradient Descent).\n",
    "\n",
    "See the `01_food_inspections_data_prep` notebook for information about the Chicago Food Inspections Data, the license, and the various data attributes.  See the `02_census_data_prep` notebook for the US Census API terms of use.\n",
    "\n",
    "### Analysis and Models in this Notebook\n",
    "\n",
    "- Simple Logistic Regression model using scikit-learn\n",
    "- From-scratch Logistic Regression model using homemade implementation of Gradient Descent\n",
    "- Spark MlLib Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Global Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 666"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression as SklearnLogisticRegression\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import FloatType, StructType, StructField, LongType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import LogisticRegression as SparkLogisticRegression\n",
    "\n",
    "import l2_regularized_logistic_regression as nplr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Train and Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../data/X_train.gz', compression='gzip')\n",
    "X_test = pd.read_csv('../data/X_test.gz', compression='gzip')\n",
    "y_train = pd.read_csv('../data/y_train.gz', compression='gzip').values.flatten()\n",
    "y_test = pd.read_csv('../data/y_test.gz', compression='gzip').values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicate the Data to Test Model Training with More Samples\n",
    "\n",
    "Add a bit of noise to the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_binary_noise(targets):\n",
    "    \n",
    "    five_percent = int(0.05 * len(targets))\n",
    "    idx = np.random.choice(list(range(0, len(targets))), five_percent, replace=False)\n",
    "    targets[idx] = -targets[idx]\n",
    "    \n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "replications = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_temp = X_train.copy()\n",
    "X_test_temp = X_test.copy()\n",
    "for i in range(1, replications):\n",
    "    X_train = X_train.append(X_train_temp)\n",
    "    X_test = X_test.append(X_test_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = add_binary_noise(nplr.transform_target(np.repeat(y_train, replications, axis=0)))\n",
    "y_test = add_binary_noise(nplr.transform_target(np.repeat(y_test, replications, axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train_scaled = minmax_scaler.fit_transform(X_train.values)\n",
    "X_test_scaled = minmax_scaler.transform(X_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Performance of scikit-learn Logistic Regression with No Regularization\n",
    "Setting the regularization parameter to `1e8` we get effectively no regularization, as in the statsmodel API Logit model.  See this issue for details: https://github.com/scikit-learn/scikit-learn/issues/6738"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_clf = SklearnLogisticRegression(C=1e8, # https://github.com/scikit-learn/scikit-learn/issues/6738\n",
    "                                        penalty='l2',\n",
    "                                        solver='liblinear',\n",
    "                                        fit_intercept=True,\n",
    "                                        max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.28 s, sys: 410 ms, total: 5.69 s\n",
      "Wall time: 4.81 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000000.0, class_weight=None, dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=1000, multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sklearn_clf.fit(X_train_scaled, np.array([0 if x == -1 else 1 for x in y_train]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Accuracy at 0.5 Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = sklearn_clf.predict_proba(X_test_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.746629213483146"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = [1 if x > 0.5 else 0 for x in y_prob]\n",
    "np.mean(np.array([0 if x == -1 else 1 for x in y_test]) == y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"lr2\")\n",
    "         .config(\"spark.rpc.message.maxSize\", \"1024mb\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Python Version\n",
    "Based on https://github.com/jstremme/l2-regularized-logistic-regression but without regularization and vectorization of matrix operations.  Instead `py_lr_grad_descent` computes the gradient on each sample sequentially, unlike Spark which will compute the gradient on each sample but in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Algorithm for Spark RDD\n",
    "M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma,\n",
    "M. McCauley, M. J. Franklin, S. Shenker, and I. Stoica. Resilient distributed datasets: A fault-tolerant\n",
    "abstraction for in-memory cluster computing.\n",
    "In Proceedings of NSDI, pages 15â€“28, 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new X and Ys for From-Scratch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = X_train_scaled.copy()\n",
    "Xt_test = X_test_scaled.copy()\n",
    "yt = y_train.copy()\n",
    "yt_test = y_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_lr_grad(d):\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    x = np.array(d.x)\n",
    "    y = d.y\n",
    "    \n",
    "    return x * (1 / (1 + np.exp(-y * np.dot(w, x))) - 1) * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_lr_grad_descent(w, samples, n, learning_rate, n_iter):\n",
    "    \n",
    "    ws = [w.copy()]\n",
    "    for i in range(0, n_iter):\n",
    "        gradient = samples.map(spark_lr_grad).reduce(lambda a, b: a + b)\n",
    "        w -= learning_rate * gradient * 1/n\n",
    "        ws.append(w.copy())\n",
    "    \n",
    "    return ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Spark Dataframe of X Vectors and y Targets\n",
    "Transform the target variable to [1, -1] instead of [1, 0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df = pd.DataFrame(yt, columns=['y'])\n",
    "pd_df['x'] = Xt.tolist()\n",
    "df = spark.createDataFrame(pd_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[7] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = df.rdd\n",
    "samples.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = np.zeros(Xt.shape[1])\n",
    "w = np.random.uniform(-1, 1, (Xt.shape[1]))\n",
    "n = samples.count()\n",
    "learning_rate = 1.0\n",
    "n_iter = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pd_df\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ws = spark_lr_grad_descent(w, samples, n, learning_rate, n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = nplr.predict_all(ws, Xt_test)\n",
    "nplr.plot_accuracies(preds, yt_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def py_lr_grad(w, x, y):\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    x = np.array(x)\n",
    "    y = y.copy()\n",
    "    \n",
    "    return x * (1 / (1 + np.exp(-y * np.dot(w, x))) - 1) * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Python Version\n",
    "Based on https://github.com/jstremme/l2-regularized-logistic-regression but without regularization and vectorization of matrix operations.  Instead `py_lr_grad_descent` computes the gradient on each sample sequentially, unlike Spark which will compute the gradient on each sample but in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def py_lr_grad_descent(w, X, y, n, learning_rate, n_iter, v):\n",
    "    \n",
    "    ws = [w.copy()]\n",
    "    for i in range(0, n_iter):\n",
    "    \n",
    "        gradient_total = 0\n",
    "        for j in range(0, n, 1): # replaced with map and reduce in spark\n",
    "            x_j = X[j]\n",
    "            y_j = y[j]\n",
    "            gradient_total += py_lr_grad(w, x_j, y_j)\n",
    "\n",
    "        w -= learning_rate * gradient_total * 1/n\n",
    "        ws.append(w.copy())\n",
    "        \n",
    "        if v:\n",
    "            print('Objective function at iteration {}: {}'.format(i, nplr.obj(X, y, beta=w, lambda_penalty=0)))\n",
    "    \n",
    "    return ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialized_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = np.zeros(Xt.shape[1])\n",
    "w = np.random.uniform(-1, 1, (Xt.shape[1]))\n",
    "n = Xt.shape[0]\n",
    "learning_rate = 1.0\n",
    "n_iter = 50\n",
    "v = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ws = py_lr_grad_descent(w, Xt, yt, n, learning_rate, n_iter, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = nplr.predict_all(ws, Xt_test)\n",
    "nplr.plot_accuracies(preds, yt_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelized Numpy Version\n",
    "Implementation from https://github.com/jstremme/l2-regularized-logistic-regression.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ws = nplr.l2_log_reg(Xt, yt, lambda_penalty=0, eps=0.001, v=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = nplr.predict_all(ws, Xt_test)\n",
    "nplr.plot_accuracies(preds, yt_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark MlLib Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = spark.createDataFrame(nplr.to_df_with_class(X_train_scaled,\n",
    "                                                       np.array([0 if x == -1 else 1 for x in y_train])))\n",
    "test_df = spark.createDataFrame(nplr.to_df_with_class(X_test_scaled,\n",
    "                                                      np.array([0 if x == -1 else 1 for x in y_test])))\n",
    "target_column = 'class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_assemble = [item for item in train_df.columns if item != target_column]\n",
    "assembler = VectorAssembler(inputCols=to_assemble, outputCol='features')\n",
    "train_vector = assembler.transform(train_df)\n",
    "test_vector = assembler.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vector.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = SparkLogisticRegression(labelCol=target_column, featuresCol='features', regParam=0,\n",
    "                             tol=0.001, standardization=False, fitIntercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lr = lr.fit(train_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.transform(test_vector).select('prediction').rdd.map(lambda x: x.prediction).collect()\n",
    "np.mean(np.array([0 if x == -1 else 1 for x in y_test]) == y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
