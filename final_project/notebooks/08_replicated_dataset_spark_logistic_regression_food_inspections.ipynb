{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Logistic Regression with Replicated Data\n",
    "This notebook reads the data splits from `04_create_data_splits` to apply a Logistic Regression model in Python and a from-scratch Logistic Regression model in Spark (with a homemade implementation of Gradient Descent).\n",
    "\n",
    "See the `01_food_inspections_data_prep` notebook for information about the Chicago Food Inspections Data, the license, and the various data attributes.  See the `02_census_data_prep` notebook for the US Census API terms of use.\n",
    "\n",
    "### Analysis and Models in this Notebook\n",
    "\n",
    "- Simple Logistic Regression model using scikit-learn\n",
    "- From-scratch Logistic Regression model using homemade implementation of Gradient Descent\n",
    "- Spark MlLib Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Global Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 666"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression as SklearnLogisticRegression\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import FloatType, StructType, StructField, LongType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import LogisticRegression as SparkLogisticRegression\n",
    "\n",
    "import l2_regularized_logistic_regression as nplr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Train and Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../data/X_train.gz', compression='gzip')\n",
    "X_test = pd.read_csv('../data/X_test.gz', compression='gzip')\n",
    "y_train = pd.read_csv('../data/y_train.gz', compression='gzip').values.flatten()\n",
    "y_test = pd.read_csv('../data/y_test.gz', compression='gzip').values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicate the Data to Test Model Training with More Samples\n",
    "\n",
    "Add a bit of noise to the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_binary_noise(targets):\n",
    "    \n",
    "    five_percent = int(0.05 * len(targets))\n",
    "    idx = np.random.choice(list(range(0, len(targets))), five_percent, replace=False)\n",
    "    targets[idx] = -targets[idx]\n",
    "    \n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "replications = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_temp = X_train.copy()\n",
    "X_test_temp = X_test.copy()\n",
    "for i in range(1, replications):\n",
    "    X_train = X_train.append(X_train_temp)\n",
    "    X_test = X_test.append(X_test_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = add_binary_noise(nplr.transform_target(np.repeat(y_train, replications, axis=0)))\n",
    "y_test = add_binary_noise(nplr.transform_target(np.repeat(y_test, replications, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-267450"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train_scaled = minmax_scaler.fit_transform(X_train.values)\n",
    "X_test_scaled = minmax_scaler.transform(X_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Performance of scikit-learn Logistic Regression with No Regularization\n",
    "Setting the regularization parameter to `1e8` we get effectively no regularization, as in the statsmodel API Logit model.  See this issue for details: https://github.com/scikit-learn/scikit-learn/issues/6738"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_clf = SklearnLogisticRegression(C=1e8, # https://github.com/scikit-learn/scikit-learn/issues/6738\n",
    "                                        penalty='l2',\n",
    "                                        solver='liblinear',\n",
    "                                        fit_intercept=True,\n",
    "                                        max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.54 s, sys: 81.8 ms, total: 3.62 s\n",
      "Wall time: 3.62 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000000.0, class_weight=None, dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=1000, multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sklearn_clf.fit(X_train_scaled, np.array([0 if x == -1 else 1 for x in y_train]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Accuracy at 0.5 Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = sklearn_clf.predict_proba(X_test_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7465093632958801"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = [1 if x > 0.5 else 0 for x in y_prob]\n",
    "np.mean(np.array([0 if x == -1 else 1 for x in y_test]) == y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.appName('lr').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Python Version\n",
    "Based on https://github.com/jstremme/l2-regularized-logistic-regression but without regularization and vectorization of matrix operations.  Instead `py_lr_grad_descent` computes the gradient on each sample sequentially, unlike Spark which will compute the gradient on each sample but in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Algorithm for Spark RDD\n",
    "M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma,\n",
    "M. McCauley, M. J. Franklin, S. Shenker, and I. Stoica. Resilient distributed datasets: A fault-tolerant\n",
    "abstraction for in-memory cluster computing.\n",
    "In Proceedings of NSDI, pages 15â€“28, 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new X and Ys for From-Scratch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = X_train_scaled.copy()\n",
    "Xt_test = X_test_scaled.copy()\n",
    "yt = y_train.copy()\n",
    "yt_test = y_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_lr_grad(d):\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    x = np.array(d.x)\n",
    "    y = d.y\n",
    "    \n",
    "    return x * (1 / (1 + np.exp(-y * np.dot(w, x))) - 1) * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_lr_grad_descent(w, samples, n, learning_rate, n_iter):\n",
    "    \n",
    "    ws = [w.copy()]\n",
    "    for i in range(0, n_iter):\n",
    "        gradient = samples.map(spark_lr_grad).reduce(lambda a, b: a + b)\n",
    "        w -= learning_rate * gradient * 1/n # set equal to minus instead of subtracting?\n",
    "        ws.append(w.copy())\n",
    "    \n",
    "    return ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Spark Dataframe of X Vectors and y Targets\n",
    "Transform the target variable to [1, -1] instead of [1, 0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df = pd.DataFrame(yt, columns=['y'])\n",
    "pd_df['x'] = Xt.tolist()\n",
    "df = spark.createDataFrame(pd_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[7] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = df.rdd\n",
    "samples.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = np.zeros(Xt.shape[1])\n",
    "w = np.random.uniform(-1, 1, (Xt.shape[1]))\n",
    "n = samples.count()\n",
    "learning_rate = 1.0\n",
    "n_iter = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ws = spark_lr_grad_descent(w, samples, n, learning_rate, n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = nplr.predict_all(ws, Xt_test)\n",
    "nplr.plot_accuracies(preds, yt_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def py_lr_grad(w, x, y):\n",
    "    \n",
    "    x = np.array(x)\n",
    "    y = y.copy()\n",
    "    \n",
    "    return x * (1 / (1 + np.exp(-y * np.dot(w, x))) - 1) * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Python Version\n",
    "Based on https://github.com/jstremme/l2-regularized-logistic-regression but without regularization and vectorization of matrix operations.  Instead `py_lr_grad_descent` computes the gradient on each sample sequentially, unlike Spark which will compute the gradient on each sample but in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def py_lr_grad_descent(w, X, y, n, learning_rate, n_iter, v):\n",
    "    \n",
    "    ws = [w.copy()]\n",
    "    for i in range(0, n_iter):\n",
    "    \n",
    "        gradient_total = 0\n",
    "        for j in range(0, n, 1): # replaced with map and reduce in spark\n",
    "            x_j = X[j]\n",
    "            y_j = y[j]\n",
    "            gradient_total += py_lr_grad(w, x_j, y_j)\n",
    "\n",
    "        w -= learning_rate * gradient_total * 1/n # set equal to minus instead of subtracting?\n",
    "        ws.append(w.copy())\n",
    "        \n",
    "        if v:\n",
    "            print('Objective function at iteration {}: {}'.format(i, nplr.obj(X, y, beta=w, lambda_penalty=0)))\n",
    "    \n",
    "    return ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialized_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = np.zeros(Xt.shape[1])\n",
    "w = np.random.uniform(-1, 1, (Xt.shape[1]))\n",
    "n = Xt.shape[0]\n",
    "learning_rate = 1.0\n",
    "n_iter = 50\n",
    "v = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective function at iteration 0: 0.6513220906240923\n",
      "Objective function at iteration 1: 0.6320092840647402\n",
      "Objective function at iteration 2: 0.6235725671280868\n",
      "Objective function at iteration 3: 0.6193024214201217\n",
      "Objective function at iteration 4: 0.6167488889083815\n",
      "Objective function at iteration 5: 0.6149493300523946\n",
      "Objective function at iteration 6: 0.6135037083512165\n",
      "Objective function at iteration 7: 0.6122388371361919\n",
      "Objective function at iteration 8: 0.6110773382231272\n",
      "Objective function at iteration 9: 0.6099836342602123\n",
      "Objective function at iteration 10: 0.6089408007988221\n",
      "Objective function at iteration 11: 0.607940369547309\n",
      "Objective function at iteration 12: 0.606977751878662\n",
      "Objective function at iteration 13: 0.6060501591233933\n",
      "Objective function at iteration 14: 0.6051556499486282\n",
      "Objective function at iteration 15: 0.6042926919338913\n",
      "Objective function at iteration 16: 0.6034599592967009\n",
      "Objective function at iteration 17: 0.6026562394795829\n",
      "Objective function at iteration 18: 0.601880389997081\n",
      "Objective function at iteration 19: 0.601131318478918\n",
      "Objective function at iteration 20: 0.6004079733945651\n",
      "Objective function at iteration 21: 0.5997093396732692\n",
      "Objective function at iteration 22: 0.5990344365481822\n",
      "Objective function at iteration 23: 0.5983823163942675\n",
      "Objective function at iteration 24: 0.5977520639953603\n",
      "Objective function at iteration 25: 0.5971427959825711\n",
      "Objective function at iteration 26: 0.5965536603272199\n",
      "Objective function at iteration 27: 0.5959838358361008\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ws = py_lr_grad_descent(w, Xt, yt, n, learning_rate, n_iter, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = nplr.predict_all(ws, Xt_test)\n",
    "nplr.plot_accuracies(preds, yt_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelized Numpy Version\n",
    "Implementation from https://github.com/jstremme/l2-regularized-logistic-regression.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ws = nplr.l2_log_reg(Xt, yt, lambda_penalty=0, eps=0.001, v=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = nplr.predict_all(ws, Xt_test)\n",
    "nplr.plot_accuracies(preds, yt_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark MlLib Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = spark.createDataFrame(nplr.to_df_with_class(X_train_scaled, y_train))\n",
    "test_df = spark.createDataFrame(nplr.to_df_with_class(X_test_scaled, y_test))\n",
    "target_column = 'class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_assemble = [item for item in train_df.columns if item != target_column]\n",
    "assembler = VectorAssembler(inputCols=to_assemble, outputCol='features')\n",
    "train_vector = assembler.transform(train_df)\n",
    "test_vector = assembler.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vector.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = SparkLogisticRegression(labelCol=target_column, featuresCol='features', regParam=0,\n",
    "                             tol=0.001, standardization=False, fitIntercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lr = lr.fit(train_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.transform(test_vector).select('prediction').rdd.map(lambda x: x.prediction).collect()\n",
    "np.mean(y_test == y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
