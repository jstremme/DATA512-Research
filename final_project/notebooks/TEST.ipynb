{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Logistic Regression with Replicated Data\n",
    "This notebook reads the data splits from `04_create_data_splits` to apply a Logistic Regression model in Python and a from-scratch Logistic Regression model in Spark (with a homemade implementation of Gradient Descent).\n",
    "\n",
    "See the `01_food_inspections_data_prep` notebook for information about the Chicago Food Inspections Data, the license, and the various data attributes.  See the `02_census_data_prep` notebook for the US Census API terms of use.\n",
    "\n",
    "### Analysis and Models in this Notebook\n",
    "\n",
    "- Simple Logistic Regression model using scikit-learn\n",
    "- From-scratch Logistic Regression model using homemade implementation of Gradient Descent\n",
    "- Spark MlLib Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Global Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 666"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression as SklearnLogisticRegression\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import FloatType, StructType, StructField, LongType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import LogisticRegression as SparkLogisticRegression\n",
    "\n",
    "import l2_regularized_logistic_regression as nplr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Train and Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../data/X_train.gz', compression='gzip')\n",
    "X_test = pd.read_csv('../data/X_test.gz', compression='gzip')\n",
    "y_train = pd.read_csv('../data/y_train.gz', compression='gzip').values.flatten()\n",
    "y_test = pd.read_csv('../data/y_test.gz', compression='gzip').values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicate the Data to Test Model Training with More Samples\n",
    "\n",
    "Add a bit of noise to the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_binary_noise(targets):\n",
    "    \n",
    "    five_percent = int(0.05 * len(targets))\n",
    "    idx = np.random.choice(list(range(0, len(targets))), five_percent, replace=False)\n",
    "    targets[idx] = -targets[idx]\n",
    "    \n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "replications = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_temp = X_train.copy()\n",
    "X_test_temp = X_test.copy()\n",
    "for i in range(1, replications):\n",
    "    X_train = X_train.append(X_train_temp)\n",
    "    X_test = X_test.append(X_test_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = add_binary_noise(nplr.transform_target(np.repeat(y_train, replications, axis=0)))\n",
    "y_test = add_binary_noise(nplr.transform_target(np.repeat(y_test, replications, axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"lr2\")\n",
    "         .config(\"spark.rpc.message.maxSize\", \"1024mb\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark MlLib Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[target_column] = y_train\n",
    "X_test[target_column] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = spark.createDataFrame(X_train)\n",
    "test_df = spark.createDataFrame(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_assemble = [item for item in train_df.columns if item != target_column]\n",
    "assembler = VectorAssembler(inputCols=to_assemble, outputCol='features')\n",
    "train_vector = assembler.transform(train_df)\n",
    "test_vector = assembler.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vector.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = SparkLogisticRegression(labelCol=target_column, featuresCol='features', regParam=0,\n",
    "                             tol=0.001, standardization=True, fitIntercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lr = lr.fit(train_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.transform(test_vector).select('prediction').rdd.map(lambda x: x.prediction).collect()\n",
    "np.mean(np.array([0 if x == -1 else 1 for x in y_test]) == y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8\n",
   "language": "python",
   "name": "py368"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
